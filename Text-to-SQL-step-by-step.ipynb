{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # Hugging Face's Transformer Library\n",
        "\n",
        " **Better to run in Google Colab** \n",
        " \n",
        " Hugging Face Transformers is an open-source framework for deep learning created by Hugging Face.\n",
        " It provides APIs and tools to download state-of-the-art pre-trained models and further tune them to maximize performance.\n",
        " These models support common tasks in different modalities, such as natural language processing, computer vision, audio, and multi-modal applications.\n",
        " Using pretrained models can reduce your compute costs, carbon footprint,\n",
        " and save you the time and resources required to train a model from scratch.\n",
        "\n",
        " - https://huggingface.co/docs/transformers/index\n",
        " - https://huggingface.co/docs/hub/index\n",
        "\n",
        " Accelerate library to help users easily train a ðŸ¤— Transformers model on any type of distributed setup,\n",
        " whether it is multiple GPU's on one machine or multiple GPU's across several machines.\n",
        "\n",
        " Note: To use the LLAMA model, you need to get access to Llama first. You can do so by filling out this [form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "IW2eLcHZElq8"
      },
      "outputs": [],
      "source": [
        "# Install libraries in Google colab\n",
        "# !pip install -q transformers langchain huggingface_hub accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y6C2vOtNBJK",
        "outputId": "7a4d6a5c-17d3-495d-b6e8-68e51f40a964"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kdlx593/miniconda3/envs/text_to_sql/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /Users/kdlx593/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# we need to login to Hugging Face to have access to their inference API.\n",
        "# This step requires a free Hugging Face token.\n",
        "\n",
        "from huggingface_hub import login\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "from langchain import PromptTemplate,  LLMChain\n",
        "\n",
        "\n",
        "# Log in to Hugginb face using your token.\n",
        "login(\"YOUR API KEY\")  # Hugging face token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "a4f702fce81c41f7b4f034ebca4f05c5",
            "4ef5c61e6e604448aebff796bf60c3ff",
            "26ae7ca9c7bb4159a063df186e1b4a65",
            "0a898c1dbf2c4bfead2edf2feda4e5db",
            "57074d5b21744d63a0065c6fb22bc854",
            "b542bfb4114e41b492e86a98565754f2",
            "8d088e941c384c1291b315649aace9f0",
            "86acfbe4c1234c05bc4a750e91b60cfd",
            "99c20af80fcf49d0af131ec480a5bc15",
            "dc49834014be42ff81214f8114a7edc0",
            "1fa71757bd8a47fea752228b07e1e9a3"
          ]
        },
        "id": "o787jKFhKTFq",
        "outputId": "d0d674f3-3e7a-42af-be6e-d102a3378a5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Let's bring the Lama2 model into play. \n",
        "# Remember to get access to LAMA2 beforehand!\n",
        "# https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "# model = \"meta-llama/Llama-2-7b-chat-hf\" -> Requires access from META\n",
        "model = \"mrm8488/t5-base-finetuned-wikiSQL\"\n",
        "# Load tokenizer and model\n",
        "# Here we are loading the tokenization used for this particular model. \n",
        "# Tokenization basically treats each word (and some characters) as an object\n",
        "# which has valuable attributes used in NLP\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "# Set up text generation pipeline. This will handle many of the operations\n",
        "# for you in the background, including tokenization, loading the model, etc.\n",
        "# For now let's just leave this configurations as they are. \n",
        "pipeline = transformers.pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    # torch_dtype=torch.bfloat16,  # activate this option for llama2\n",
        "    device_map=\"auto\",  # GPU or CPU, auto for automatic selection\n",
        "    max_new_tokens = 512,\n",
        "    # do_sample=True,  # activate for llama2\n",
        "    top_k=10,\n",
        "    # num_return_sequences=1,  # activate for llama2\n",
        "    # eos_token_id=tokenizer.eos_token_id,  # activate for llama2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bzALtoh1XAAo"
      },
      "outputs": [],
      "source": [
        "# 'HuggingFacePipeline' class creates a custom pipeline for text generation, and we are passing\n",
        "# the pipeline that we defined earlier along with some model-specific keyword arguments - temperature here.\n",
        "\n",
        "temperature = 0  # give any value between 0 and 1\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=pipeline, \n",
        "    model_kwargs={\n",
        "        'temperature': temperature\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "sGi8jVecKkzH"
      },
      "outputs": [],
      "source": [
        "# We create the prompt that we are going to use to feed to the llm.\n",
        "template = \"\"\"\n",
        "   Transalte English to SQL from the prompt below:\n",
        "   ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "# This will allow us to just add our query and have the answer we are looking for\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Let's test it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kdlx593/miniconda3/envs/text_to_sql/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELECT Age (years) FROM table WHERE Group by store = \n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "What is the averege age of our clients, grouped by store\n",
        "\"\"\"\n",
        "print(llm_chain.run(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZC3AnCWWZCQy",
        "outputId": "b3af4620-0d9c-4739-d3e5-9c3351cf6d6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELECT COUNT Population (2001 census) FROM table WHERE City = Barcelona\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "How many people live in Barcelona\n",
        "\"\"\"\n",
        "print(llm_chain.predict(text=text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kdlx593/miniconda3/envs/text_to_sql/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELECT COUNT Population (2001 census) FROM table WHERE City = Barcelona\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "How many people live in Barcelona\n",
        "\"\"\"\n",
        "print(llm_chain.run(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4hoPxmnhvmzr"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kdlx593/miniconda3/envs/text_to_sql/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:377: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SELECT MAX Spend per day by client FROM table ExampeTable3\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "I want to know the maximum Spend per day by client in table ExampeTable3, \n",
        "\"\"\"\n",
        "print(llm_chain.run(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# What if I don't want to import and load the model to disc. \n",
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/mrm8488/t5-base-finetuned-wikiSQL\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\t\n",
        "output = query({\n",
        "\t\"inputs\": \" translate English to SQL: How many people live in Spain\",\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'SELECT COUNT Population FROM table WHERE Country = Spain'}]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a898c1dbf2c4bfead2edf2feda4e5db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc49834014be42ff81214f8114a7edc0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1fa71757bd8a47fea752228b07e1e9a3",
            "value": " 2/2 [01:23&lt;00:00, 38.45s/it]"
          }
        },
        "1fa71757bd8a47fea752228b07e1e9a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26ae7ca9c7bb4159a063df186e1b4a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86acfbe4c1234c05bc4a750e91b60cfd",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99c20af80fcf49d0af131ec480a5bc15",
            "value": 2
          }
        },
        "4ef5c61e6e604448aebff796bf60c3ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b542bfb4114e41b492e86a98565754f2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8d088e941c384c1291b315649aace9f0",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "57074d5b21744d63a0065c6fb22bc854": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86acfbe4c1234c05bc4a750e91b60cfd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d088e941c384c1291b315649aace9f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99c20af80fcf49d0af131ec480a5bc15": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4f702fce81c41f7b4f034ebca4f05c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ef5c61e6e604448aebff796bf60c3ff",
              "IPY_MODEL_26ae7ca9c7bb4159a063df186e1b4a65",
              "IPY_MODEL_0a898c1dbf2c4bfead2edf2feda4e5db"
            ],
            "layout": "IPY_MODEL_57074d5b21744d63a0065c6fb22bc854"
          }
        },
        "b542bfb4114e41b492e86a98565754f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc49834014be42ff81214f8114a7edc0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
